{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f592fbb0",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "_Tokenization_ is the process of converting a body of text into individual _tokens_, e.g., words and punctuation characters. This is the first step for most Natural Language Processing (NLP) tasks, including preparing data for training an LLM. Let's see how it's done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f17e49",
   "metadata": {},
   "source": [
    "## Some sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc42f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test! Or is this not a test? Test it to be sure. :)\n",
      "This sample text has 61 characters.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a test! Or is this not a test? Test it to be sure. :)\"\n",
    "print(text)\n",
    "print(f\"This sample text has {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5ff799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Return a list of the substrings in the string, using sep as the separator string.\n",
      "\n",
      "  sep\n",
      "    The separator used to split the string.\n",
      "\n",
      "    When set to None (the default value), will split on any whitespace\n",
      "    character (including \\n \\r \\t \\f and spaces) and will discard\n",
      "    empty strings from the result.\n",
      "  maxsplit\n",
      "    Maximum number of splits.\n",
      "    -1 (the default value) means no limit.\n",
      "\n",
      "Splitting starts at the front of the string and works to the end.\n",
      "\n",
      "Note, str.split() is mainly useful for data that has been intentionally\n",
      "delimited.  With natural text that includes punctuation, consider using\n",
      "the regular expression module.\n",
      "\u001b[1;31mType:\u001b[0m      method_descriptor"
     ]
    }
   ],
   "source": [
    "str.split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "316aec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f88fa76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '.', ':)', '?', 'Or', 'Test', 'This', 'a', 'be', 'is', 'it', 'not', 'sure', 'test', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([.?!]|\\s)', text)\n",
    "tokens = [ item for item in tokens if item.split()] # returns true if it's not white space\n",
    "tokens = list(set(tokens))\n",
    "tokens = sorted(list(set(tokens)))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4c9b3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('!', 0), ('.', 1), (':)', 2), ('?', 3), ('Or', 4), ('Test', 5), ('This', 6), ('a', 7), ('be', 8), ('is', 9), ('it', 10), ('not', 11), ('sure', 12), ('test', 13), ('this', 14), ('to', 15)])\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:index for index, token in enumerate(tokens)}\n",
    "print(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d51d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ancient Egypt (Rawlinson)\n",
      "by George Rawlinson\n",
      "The Priest-Kings--Pinetem and Solomon\n",
      "SHISHAK AND HIS \n"
     ]
    }
   ],
   "source": [
    "with open(\"tokenization_example_story.txt\", 'r') as f:\n",
    "    raw = f.read()\n",
    "print(raw[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0cfa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', '\"Administrator', '\"Chief', '\"Fanbearer', '\"Her-Hor', '\"Her-Hor,', '\"King', '\"Principal', '\"Royal', '\"cities', '\"colleges,\"', '\"gardens,', '\"pillared', '\"story', '\"strengthens', '\"the', '(1', '(Rawlinson)', '(Wady-el-Arish)', '(ib', ',', '.', '1),', '1,', '16)', '16-19),', '20)', '21-24);', '28,', '29):', '3)', 'A', 'AND', 'According', 'After', 'Amenhotep', \"Amenhotep's;\", 'Ammon', 'Ammon,\"', 'Ammonites,', 'Ancient', 'Architect,\"', 'As', 'Assyria', 'Assyria,', 'Assyria;', 'At', 'Bedouins', 'Boaz,', 'But', 'Canaanite', 'Chron', 'Commissioner,', 'Consequently,', 'DYNASTY', 'David', \"David's\", 'Delta,', 'Edomites,', 'Egypt', \"Egypt's\", 'Egypt,', 'Egypt,\"', 'Egypt--perhaps', 'Egyptian', 'Egyptians,', 'Egyptologers', 'Euphrates', 'Finally,', 'For', 'Formerly,', 'From', 'George', 'Gezer,', 'Granaries,\"', 'Great', 'Gush,\"', 'HIS', 'He', 'Hebrew', 'Heliopolis,', 'Her-hor', 'Herhor', \"Herhor's\", 'Hesi-em-Kheb,', 'High-Priest', 'His', 'Hittites', 'Holies,', 'Holy', 'Hor-pa-seb-en-sha,', 'Horus', 'II', 'III', 'IX', 'If', 'Imperfections', 'In', 'Is', 'It']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([.?!]|\\s)', raw)\n",
    "tokens = [ item for item in tokens if item.split()] # returns true if it's not white space\n",
    "tokens = list(set(tokens))\n",
    "tokens = sorted(tokens)\n",
    "print(tokens[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
