{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1966e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beb9bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4654700",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = 'So threatening had become the general aspect of affairs, that the king thought it prudent to send his son' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dfface9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2396, 11123, 550, 1716, 262, 2276, 4843, 286, 9674, 11, 326, 262, 5822, 1807, 340, 34998, 284, 3758, 465, 3367]\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer.encode(raw)\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc980ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So threatening had become the general aspect of affairs, that the king thought it prudent to send his son\n"
     ]
    }
   ],
   "source": [
    "ret = tokenizer.decode(enc)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a46c8733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.4372e-03,  5.4385e-01,  7.5320e-04,  5.5419e-01, -2.4587e+00,\n",
      "         -2.5192e-01,  6.1233e-01,  3.2426e-01],\n",
      "        [-5.0827e-01,  8.9565e-01, -5.3059e-01,  4.1679e-01, -6.1498e-01,\n",
      "         -1.5292e-01, -4.9243e-01, -2.1665e-01],\n",
      "        [-7.5124e-01, -1.7398e+00,  1.1536e+00,  6.6208e-01, -3.6198e-01,\n",
      "          8.8740e-01, -1.2515e+00,  9.2859e-01],\n",
      "        [-1.1090e+00, -1.5436e+00,  1.5929e+00,  7.5034e-01,  8.8006e-02,\n",
      "          6.3715e-01, -2.3153e-01, -1.2820e+00]], requires_grad=True)\n",
      "tensor([[ 2.4372e-03,  5.4385e-01,  7.5320e-04,  5.5419e-01, -2.4587e+00,\n",
      "         -2.5192e-01,  6.1233e-01,  3.2426e-01],\n",
      "        [-5.0827e-01,  8.9565e-01, -5.3059e-01,  4.1679e-01, -6.1498e-01,\n",
      "         -1.5292e-01, -4.9243e-01, -2.1665e-01],\n",
      "        [-7.5124e-01, -1.7398e+00,  1.1536e+00,  6.6208e-01, -3.6198e-01,\n",
      "          8.8740e-01, -1.2515e+00,  9.2859e-01],\n",
      "        [-1.1090e+00, -1.5436e+00,  1.5929e+00,  7.5034e-01,  8.8006e-02,\n",
      "          6.3715e-01, -2.3153e-01, -1.2820e+00]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 4\n",
    "output_dimension = 8\n",
    "inputs = torch.nn.Embedding(vocab_size, output_dimension)\n",
    "print(inputs.weight)\n",
    "\n",
    "inputs = inputs.weight.data\n",
    "print(inputs) # get the same answer without the 'requires_grad+True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5d52281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0635544c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0024372003972530365, 0.5438529849052429, 0.000753197877202183, 0.5541906356811523, -2.458731174468994, -0.25192269682884216, 0.6123308539390564, 0.3242608904838562]\n",
      "[-0.5082664489746094, 0.8956487774848938, -0.5305865406990051, 0.41678571701049805, -0.6149751543998718, -0.15291960537433624, -0.49242523312568665, -0.21665015816688538]\n",
      "[-0.7512362599372864, -1.7397701740264893, 1.153609037399292, 0.6620757579803467, -0.36197948455810547, 0.8874033689498901, -1.2514982223510742, 0.9285872578620911]\n",
      "[-1.1089531183242798, -1.5435950756072998, 1.5928676128387451, 0.7503377795219421, 0.08800597488880157, 0.6371463537216187, -0.23152561485767365, -1.2819617986679077]\n"
     ]
    }
   ],
   "source": [
    "for row in inputs:\n",
    "    print(row.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0fe3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1.1, 2.3])\n",
    "y = torch.Tensor([3.4, -2.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c9b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.0900)\n",
      "-1.0899999999999999\n"
     ]
    }
   ],
   "source": [
    "print(torch.dot(x, y))\n",
    "\n",
    "print(1.1 * 3.4 + 2.3 * (-2.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8626e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7512, -1.7398,  1.1536,  0.6621, -0.3620,  0.8874, -1.2515,  0.9286])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[2]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5052374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3790)\n",
      "tensor(-1.0105)\n",
      "tensor(8.7073)\n",
      "tensor(5.4858)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "    print(torch.dot(query, inputs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b649c5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3790, -1.0105,  8.7073,  5.4858])\n"
     ]
    }
   ],
   "source": [
    "att_scores2 = torch.zeros(len(inputs))\n",
    "for i in range(len(inputs)):\n",
    "    att_scores2[i] = torch.dot(query, inputs[i])\n",
    "\n",
    "print(att_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73f68bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0884e-04, 5.7878e-05, 9.6148e-01, 3.8357e-02])\n"
     ]
    }
   ],
   "source": [
    "att_weights2 = torch.softmax(att_scores2, dim = 0) # torch.exp(x)/ torch.exp(x).sum()\n",
    "print(att_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24d10460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_weights2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffd44d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7649, -1.7318,  1.1702,  0.6654, -0.3450,  0.8776, -1.2121,  0.8437])\n"
     ]
    }
   ],
   "source": [
    "context_vector2 = torch.zeros(query.shape)\n",
    "for i in range(len(att_weights2)):\n",
    "    context_vector2 += att_weights2[i] * inputs[i]\n",
    "print(context_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1931d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4372e-03,  5.4385e-01,  7.5320e-04,  5.5419e-01, -2.4587e+00,\n",
      "         -2.5192e-01,  6.1233e-01,  3.2426e-01],\n",
      "        [-5.0827e-01,  8.9565e-01, -5.3059e-01,  4.1679e-01, -6.1498e-01,\n",
      "         -1.5292e-01, -4.9243e-01, -2.1665e-01],\n",
      "        [-7.5124e-01, -1.7398e+00,  1.1536e+00,  6.6208e-01, -3.6198e-01,\n",
      "          8.8740e-01, -1.2515e+00,  9.2859e-01],\n",
      "        [-1.1090e+00, -1.5436e+00,  1.5929e+00,  7.5034e-01,  8.8006e-02,\n",
      "          6.3715e-01, -2.3153e-01, -1.2820e+00]])\n",
      "tensor([[ 2.4372e-03, -5.0827e-01, -7.5124e-01, -1.1090e+00],\n",
      "        [ 5.4385e-01,  8.9565e-01, -1.7398e+00, -1.5436e+00],\n",
      "        [ 7.5320e-04, -5.3059e-01,  1.1536e+00,  1.5929e+00],\n",
      "        [ 5.5419e-01,  4.1679e-01,  6.6208e-01,  7.5034e-01],\n",
      "        [-2.4587e+00, -6.1498e-01, -3.6198e-01,  8.8006e-02],\n",
      "        [-2.5192e-01, -1.5292e-01,  8.8740e-01,  6.3715e-01],\n",
      "        [ 6.1233e-01, -4.9243e-01, -1.2515e+00, -2.3153e-01],\n",
      "        [ 3.2426e-01, -2.1665e-01,  9.2859e-01, -1.2820e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(inputs.T) # flips axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "252805f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.1918,  1.8952, -0.3790, -1.3595],\n",
      "        [ 1.8952,  2.2068, -1.0105, -1.1111],\n",
      "        [-0.3790, -1.0105,  8.7073,  5.4858],\n",
      "        [-1.3595, -1.1111,  5.4858,  8.8234]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = inputs @ inputs.T\n",
    "print(attention_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b987295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9432e-01, 4.9802e-03, 5.1234e-04, 1.9219e-04],\n",
      "        [4.0491e-01, 5.5290e-01, 2.2151e-02, 2.0032e-02],\n",
      "        [1.0884e-04, 5.7878e-05, 9.6148e-01, 3.8357e-02],\n",
      "        [3.6510e-05, 4.6805e-05, 3.4300e-02, 9.6562e-01]])\n"
     ]
    }
   ],
   "source": [
    "attention_weights = torch.softmax( attention_scores, dim = -1 )\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fad30954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "print(attention_weights[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40fdc414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.0594e-04,  5.4403e-01, -9.9635e-04,  5.5360e-01, -2.4480e+00,\n",
      "         -2.5068e-01,  6.0571e-01,  3.2157e-01],\n",
      "        [-3.1889e-01,  6.4596e-01, -2.3560e-01,  4.8454e-01, -1.3419e+00,\n",
      "         -1.5414e-01, -5.6682e-02,  6.4005e-03],\n",
      "        [-7.6486e-01, -1.7318e+00,  1.1702e+00,  6.6544e-01, -3.4496e-01,\n",
      "          8.7762e-01, -1.2121e+00,  8.4367e-01],\n",
      "        [-1.0966e+00, -1.5501e+00,  1.5776e+00,  7.4729e-01,  7.2446e-02,\n",
      "          6.4566e-01, -2.6649e-01, -1.2060e+00]])\n"
     ]
    }
   ],
   "source": [
    "context_vectors = attention_weights @ inputs\n",
    "print(context_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
