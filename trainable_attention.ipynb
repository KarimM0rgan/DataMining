{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "674bd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4\n",
    "output_dimension = 8\n",
    "\n",
    "inputs = torch.nn.Embedding(vocab_size, output_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.0298, -1.3108, -0.7712,  0.0066,  0.3956,  0.2575,  1.1512, -1.7080],\n",
       "        [ 0.9042, -0.2926, -0.6147, -1.3084, -0.5278, -0.1160, -1.7883,  1.4322],\n",
       "        [-2.1895, -1.3615,  1.3255,  0.5632,  0.0529,  0.4835,  1.1977,  0.8252],\n",
       "        [-1.6999, -0.2699, -0.9496, -1.0292,  0.3475, -0.3594, -0.5015, -0.3627]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0298, -1.3108, -0.7712,  0.0066,  0.3956,  0.2575,  1.1512, -1.7080],\n",
       "        [ 0.9042, -0.2926, -0.6147, -1.3084, -0.5278, -0.1160, -1.7883,  1.4322],\n",
       "        [-2.1895, -1.3615,  1.3255,  0.5632,  0.0529,  0.4835,  1.1977,  0.8252],\n",
       "        [-1.6999, -0.2699, -0.9496, -1.0292,  0.3475, -0.3594, -0.5015, -0.3627]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.data # without 'requires_grad=True'\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "208c3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimensions\n",
    "d_in = 8 # inputs.shape[i]\n",
    "d_out = 6 # preferred output size\n",
    "\n",
    "# create weight matrices\n",
    "W_q = torch.nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "W_k = torch.nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "W_v = torch.nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7134, -0.7977, -1.7937, -2.0356, -0.7256,  0.0425],\n",
       "        [-2.8993, -1.5911, -0.3089, -0.2292, -0.9948, -2.1418],\n",
       "        [ 0.9768,  2.2162, -1.8151,  0.6648,  0.7611,  1.3483],\n",
       "        [-3.3262, -2.6548, -2.0784, -3.3578, -2.4539, -2.2918]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose an input vector and transform it into our query vector using W_q\n",
    "# Note that the output has the preferred size\n",
    "query = inputs @ W_q\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "28dcd2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[-0.4895, -1.3706, -1.4372, -1.4638, -1.1333, -0.6843],\n",
      "        [-0.1225, -1.6693, -1.1195, -1.1018, -1.4315, -2.1682],\n",
      "        [-0.0236,  0.4821,  0.4305,  1.5627,  0.4584,  2.7486],\n",
      "        [-0.7373, -2.1763, -1.6892, -1.9651, -2.7166, -2.4873]])\n",
      "Values: tensor([[-1.2902, -0.9184, -0.8885, -0.9777, -1.7442, -1.7779],\n",
      "        [-0.7943, -0.7326, -2.6527, -0.8318, -0.9299, -1.4858],\n",
      "        [ 0.8355,  0.1140,  0.0805, -1.3144,  1.7866,  0.2797],\n",
      "        [-1.8394, -1.5204, -2.6693, -2.3475, -2.7289, -2.0297]])\n"
     ]
    }
   ],
   "source": [
    "# calculate attention scores using the keys generated by W_k:\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\", keys)\n",
    "print(\"Values:\", values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.7933,   6.6164,  -4.5368,  11.1575],\n",
       "        [  6.9725,   9.6773,  -7.5327,  14.6023],\n",
       "        [ -3.6652,  -6.5325,   5.3576,  -9.2048],\n",
       "        [ 17.5183,  19.3472, -14.7673,  30.7058]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores are how important a token is. Ex. when translating word by word,\n",
    "# the word being translated gets the highest att score. the others get scores too but not as big as the meant word\n",
    "attention_scores = query @ keys.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1123b",
   "metadata": {},
   "source": [
    "Attention weights are different as they refer to the input and other inputs with respect to the highest attention (query) score input\n",
    "Ex. a23 --> a is the name of the weight, 2 is the highest score, and 3 is the position of the input with respect to the highest, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e2383b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7941e-01, 1.1096e-01, 1.1687e-03, 7.0846e-01],\n",
       "        [3.7665e-02, 1.1363e-01, 1.0097e-04, 8.4860e-01],\n",
       "        [2.4271e-02, 7.5286e-03, 9.6567e-01, 2.5288e-03],\n",
       "        [4.5259e-03, 9.5494e-03, 8.5402e-09, 9.8592e-01]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dividing by the root of shape of a row in keys to keep them in a reasonable range\n",
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "50eab267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ea06dee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6218, -1.3231, -2.3448, -1.9323, -2.3474, -1.9215],\n",
       "        [-1.6997, -1.4081, -2.6001, -2.1236, -2.4869, -1.9582],\n",
       "        [ 0.7648,  0.0784,  0.0295, -1.3052,  1.6690,  0.2106],\n",
       "        [-1.8269, -1.5102, -2.6611, -2.3268, -2.7073, -2.0234]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attention_weights @ values\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8c7c16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a first version of a SimpleAttention class:\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    # create weight matrices:\n",
    "    self.W_q = nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "    self.W_k = nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "    self.W_v = nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "\n",
    "  # x = embedding vectors (inputs)\n",
    "  def forward( self, x ):\n",
    "    queries = x @ self.W_q\n",
    "    keys = x @ self.W_k\n",
    "    values = x @ self.W_v\n",
    "\n",
    "    scores = queries @ keys.T\n",
    "    weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "    context = weights @ values\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7ca78c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how to use this class:\n",
    "# instantiate an instance of it:\n",
    "simple = SimpleAttention( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ee10547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.7882, 0.5257, 0.9291, 0.2225, 0.9376, 0.6388],\n",
       "        [0.3784, 0.7168, 0.4756, 0.7102, 0.2398, 0.6359],\n",
       "        [0.0596, 0.8810, 0.2340, 0.6542, 0.7770, 0.7621],\n",
       "        [0.6871, 0.6001, 0.0829, 0.9675, 0.9467, 0.4399],\n",
       "        [0.9656, 0.0739, 0.4295, 0.5311, 0.9109, 0.7623],\n",
       "        [0.6701, 0.5710, 0.3946, 0.5935, 0.1420, 0.7334],\n",
       "        [0.1673, 0.1421, 0.8042, 0.7996, 0.5478, 0.3332],\n",
       "        [0.6309, 0.7429, 0.7442, 0.1275, 0.8110, 0.2621]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "efbbf836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3679, -3.0142, -2.6160, -2.6218, -3.5951, -2.6449],\n",
       "        [-2.3093, -2.9916, -2.5485, -2.5144, -3.4955, -2.5765],\n",
       "        [-0.8409, -0.2411, -0.7559,  0.1175, -0.2481, -0.5641],\n",
       "        [-2.4182, -3.0579, -2.6747, -2.6560, -3.6653, -2.6890]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0e180fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a second version of a SimpleAttention class ;\n",
    "# it uses nn.Linear to do things more efficiently and gives better training results\n",
    "\n",
    "class SimpleAttentionV2( nn.Module ):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    # create weight matrices:\n",
    "    self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "  # x = embedding vectors (inputs)\n",
    "  def forward( self, x ):\n",
    "    queries = self.W_q( x )\n",
    "    keys = self.W_k( x )\n",
    "    values = self.W_v( x )\n",
    "\n",
    "    scores = queries @ keys.T\n",
    "    weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "    context = weights @ values\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ce88c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how to use this class:\n",
    "# instantiate an instance of it:\n",
    "simple = SimpleAttentionV2( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a21b8187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3019, -0.2753,  0.2304,  0.3721, -0.3348,  0.3127],\n",
       "        [ 0.3258, -0.2385,  0.2726,  0.3736, -0.2890,  0.2899],\n",
       "        [ 0.2462, -0.1964,  0.2366,  0.4313, -0.2747,  0.2815],\n",
       "        [ 0.0653, -0.0971,  0.0548,  0.4059, -0.2953,  0.2586]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456ebf2",
   "metadata": {},
   "source": [
    "- The problem with this is that each context vector uses information from ALL of the embedding vectors\n",
    "- In practice, we should only use information about the preceding embedding vectors\n",
    "- To accomplish this, we'll implement causal attention AKA masked attention\n",
    "- It briefly means hiding future words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bb7c14c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1752, 0.2578, 0.2785, 0.2885],\n",
       "        [0.2506, 0.2582, 0.2851, 0.2061],\n",
       "        [0.2414, 0.2247, 0.2271, 0.3068],\n",
       "        [0.1990, 0.3362, 0.1744, 0.2905]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a hack to get some example weights to work with!\n",
    "# weights = simple( inputs ) # The hack didn't work :(\n",
    "\n",
    "queries = simple.W_q(inputs)\n",
    "keys = simple.W_k(inputs)\n",
    "values = simple.W_v(inputs)\n",
    "\n",
    "scores = queries @ keys.T\n",
    "weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a3d61868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that these have already been normalized:\n",
    "weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc47e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking method #1: we soft max first then mask\n",
    "simple_mask = torch.tril( torch.ones( weights.shape[0], weights.shape[0] ) ) #Triangular mask: returns the lower triangular part\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9f55eb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1752, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2506, 0.2582, 0.0000, 0.0000],\n",
       "        [0.2414, 0.2247, 0.2271, 0.0000],\n",
       "        [0.1990, 0.3362, 0.1744, 0.2905]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = weights * simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d8598a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1752, 0.5088, 0.6932, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e15ffec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1752],\n",
       "        [0.5088],\n",
       "        [0.6932],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we need to normalize the masked_weights so that each row has sum 1 as it is good for optimization\n",
    "# What this code does -> simple_mask / row_sums\n",
    "row_sums = masked_weights.sum( dim=-1, keepdim=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ae83a25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = masked_weights / row_sums\n",
    "masked_weights.sum( dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "94afa496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4925, 0.5075, 0.0000, 0.0000],\n",
       "        [0.3483, 0.3242, 0.3276, 0.0000],\n",
       "        [0.1990, 0.3362, 0.1744, 0.2905]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd4621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking method #2\n",
    "# This way scores -> mask -> soft max\n",
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal = 1 )  #returns the upper triangular part\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "baf74a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e85769cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1752, 0.2578, 0.2785, 0.2885],\n",
       "        [0.2506, 0.2582, 0.2851, 0.2061],\n",
       "        [0.2414, 0.2247, 0.2271, 0.3068],\n",
       "        [0.1990, 0.3362, 0.1744, 0.2905]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "94abd0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1752,   -inf,   -inf,   -inf],\n",
       "        [0.2506, 0.2582,   -inf,   -inf],\n",
       "        [0.2414, 0.2247, 0.2271,   -inf],\n",
       "        [0.1990, 0.3362, 0.1744, 0.2905]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We masked the values first by hiding future values with -infinity\n",
    "weights = weights.masked_fill( mask.bool(), -torch.inf )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6e5fe938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4981, 0.5019, 0.0000, 0.0000],\n",
       "        [0.3368, 0.3312, 0.3320, 0.0000],\n",
       "        [0.2370, 0.2719, 0.2313, 0.2598]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, every row sums up to 1\n",
    "masked_weights = torch.softmax( weights, dim=-1 )\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9da7ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropout Mask\n",
    "# idea: randomly select some data to leave out to avoid overfitting\n",
    "dropout = nn.Dropout( 0.5 ) # 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "068c12b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 2., 2., 2., 0.],\n",
       "        [2., 0., 0., 0., 2., 0.],\n",
       "        [0., 0., 2., 0., 2., 0.],\n",
       "        [0., 2., 0., 2., 2., 0.],\n",
       "        [2., 0., 2., 2., 2., 2.],\n",
       "        [2., 2., 0., 2., 2., 2.]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout( torch.ones(6,6) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "05a52983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to be able to give our LLM batches of input\n",
    "# for example:\n",
    "batches = torch.stack( (inputs, inputs), dim=0) #stack 2 inputs on top of eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5b05a630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0298, -1.3108, -0.7712,  0.0066,  0.3956,  0.2575,  1.1512,\n",
       "          -1.7080],\n",
       "         [ 0.9042, -0.2926, -0.6147, -1.3084, -0.5278, -0.1160, -1.7883,\n",
       "           1.4322],\n",
       "         [-2.1895, -1.3615,  1.3255,  0.5632,  0.0529,  0.4835,  1.1977,\n",
       "           0.8252],\n",
       "         [-1.6999, -0.2699, -0.9496, -1.0292,  0.3475, -0.3594, -0.5015,\n",
       "          -0.3627]],\n",
       "\n",
       "        [[-1.0298, -1.3108, -0.7712,  0.0066,  0.3956,  0.2575,  1.1512,\n",
       "          -1.7080],\n",
       "         [ 0.9042, -0.2926, -0.6147, -1.3084, -0.5278, -0.1160, -1.7883,\n",
       "           1.4322],\n",
       "         [-2.1895, -1.3615,  1.3255,  0.5632,  0.0529,  0.4835,  1.1977,\n",
       "           0.8252],\n",
       "         [-1.6999, -0.2699, -0.9496, -1.0292,  0.3475, -0.3594, -0.5015,\n",
       "          -0.3627]]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "b8667e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape\n",
    "# The output means 2 inputs, each tensor has 4 tokens, and each token is 8-dim vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0f2af53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class needs to handle batches of input!\n",
    "\n",
    "class CausalAttention( nn.Module ):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.d_out = d_out\n",
    "    # create weight matrices:\n",
    "    self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "    \n",
    "    # include dropout:\n",
    "    self.dropout = nn.Dropout( dropout )\n",
    "    \n",
    "    # use the following to manage memory efficiently\n",
    "    # When passing this to a GPU, it's better because GPUs dont read tensors\n",
    "    self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu( torch.ones(context_length, context_length), diagonal = 1 )\n",
    "    )\n",
    "\n",
    "  # x = embedding vectors (inputs)\n",
    "  def forward( self, x ):\n",
    "    b, num_tokens, d_in = x.shape # b is batch size (num inputs)\n",
    "    queries = self.W_q( x )\n",
    "    keys = self.W_k( x )\n",
    "    values = self.W_v( x )\n",
    "\n",
    "    scores = queries @ keys.transpose(1,2)\n",
    "    scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # inplace operation for better efficiency\n",
    "    weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "    weights = self.dropout( weights )\n",
    "    \n",
    "    context = weights @ values\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "61e92040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a causal attention mechanism:\n",
    "causal = CausalAttention( d_in=8, d_out=6, context_length=4, dropout=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d910ad73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4548,  0.0840,  0.1201,  0.2948,  0.1458,  0.0185],\n",
       "         [ 0.0907,  0.3469,  0.2073,  0.3515, -0.3509, -0.2936],\n",
       "         [-0.0654,  0.1142,  0.2091,  0.1274, -0.0935, -0.0815],\n",
       "         [-0.0398,  0.1362,  0.0924,  0.1151,  0.0739, -0.1267]],\n",
       "\n",
       "        [[-0.4548,  0.0840,  0.1201,  0.2948,  0.1458,  0.0185],\n",
       "         [ 0.0907,  0.3469,  0.2073,  0.3515, -0.3509, -0.2936],\n",
       "         [-0.0654,  0.1142,  0.2091,  0.1274, -0.0935, -0.0815],\n",
       "         [-0.0398,  0.1362,  0.0924,  0.1151,  0.0739, -0.1267]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "88fd5462",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Parameter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[200], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# everything below is just to show what happens with batches\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m queries\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Parameter' object is not callable"
     ]
    }
   ],
   "source": [
    "# everything below is just to show what happens with batches\n",
    "\n",
    "queries = W_q( batches )\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dada1fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Parameter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mW_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m keys\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Parameter' object is not callable"
     ]
    }
   ],
   "source": [
    "keys = W_k( batches )\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97bcea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "keys.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "03ecd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a first pass at multi-head attention\n",
    "class MultiHeadAttention( nn.Module ):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( \n",
    "            [ CausalAttention( d_in, d_out, context_length, dropout, qkv_bias ) for _ in range(num_heads) ]\n",
    "        )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        return torch.cat( [ head(x) for head in self.heads ], dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e336d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length= 4, dropout=0, num_heads = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "72fcdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4ed92953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8212, -0.8456, -0.6401, -0.6974,  0.1105, -0.1379, -0.1625,\n",
       "           0.0825,  0.8190, -0.5085,  0.3563,  0.7604,  0.4894,  0.7208,\n",
       "           0.3837, -0.2336,  0.0953,  0.2158],\n",
       "         [-0.2423, -0.2805, -0.1383,  0.3809,  0.2915,  0.0933, -0.1245,\n",
       "           0.1647,  0.1735, -0.5619,  0.2576,  0.1994,  0.4498,  0.3860,\n",
       "           0.0192,  0.2289,  0.1943,  0.0930],\n",
       "         [-0.3217, -0.5457, -0.0944, -0.2841,  0.1091, -0.5424, -0.1049,\n",
       "          -0.0095, -0.2659,  0.1045,  0.2647,  0.4789,  0.4424,  0.0798,\n",
       "           0.3823,  0.1615,  0.2073,  0.5586],\n",
       "         [-0.4849, -0.5947, -0.2791, -0.2977, -0.0066, -0.4671, -0.2279,\n",
       "          -0.0983, -0.0127,  0.1463,  0.2048,  0.6803,  0.5510,  0.0346,\n",
       "           0.1713,  0.1727,  0.2000,  0.3906]],\n",
       "\n",
       "        [[-0.8212, -0.8456, -0.6401, -0.6974,  0.1105, -0.1379, -0.1625,\n",
       "           0.0825,  0.8190, -0.5085,  0.3563,  0.7604,  0.4894,  0.7208,\n",
       "           0.3837, -0.2336,  0.0953,  0.2158],\n",
       "         [-0.2423, -0.2805, -0.1383,  0.3809,  0.2915,  0.0933, -0.1245,\n",
       "           0.1647,  0.1735, -0.5619,  0.2576,  0.1994,  0.4498,  0.3860,\n",
       "           0.0192,  0.2289,  0.1943,  0.0930],\n",
       "         [-0.3217, -0.5457, -0.0944, -0.2841,  0.1091, -0.5424, -0.1049,\n",
       "          -0.0095, -0.2659,  0.1045,  0.2647,  0.4789,  0.4424,  0.0798,\n",
       "           0.3823,  0.1615,  0.2073,  0.5586],\n",
       "         [-0.4849, -0.5947, -0.2791, -0.2977, -0.0066, -0.4671, -0.2279,\n",
       "          -0.0983, -0.0127,  0.1463,  0.2048,  0.6803,  0.5510,  0.0346,\n",
       "           0.1713,  0.1727,  0.2000,  0.3906]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "67064b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 18])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "424fb817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # As in `CausalAttention`, for inputs where `num_tokens` exceeds `context_length`, \n",
    "        # this will result in errors in the mask creation further below. \n",
    "        # In practice, this is not a problem since the LLM (chapters 4-7) ensures that inputs  \n",
    "        # do not exceed `context_length` before reaching this forward method.\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7422c9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "47cc43b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0298, -1.3108, -0.7712,  0.0066,  0.3956,  0.2575,  1.1512,\n",
       "          -1.7080],\n",
       "         [ 0.9042, -0.2926, -0.6147, -1.3084, -0.5278, -0.1160, -1.7883,\n",
       "           1.4322],\n",
       "         [-2.1895, -1.3615,  1.3255,  0.5632,  0.0529,  0.4835,  1.1977,\n",
       "           0.8252],\n",
       "         [-1.6999, -0.2699, -0.9496, -1.0292,  0.3475, -0.3594, -0.5015,\n",
       "          -0.3627]],\n",
       "\n",
       "        [[-1.0298, -1.3108, -0.7712,  0.0066,  0.3956,  0.2575,  1.1512,\n",
       "          -1.7080],\n",
       "         [ 0.9042, -0.2926, -0.6147, -1.3084, -0.5278, -0.1160, -1.7883,\n",
       "           1.4322],\n",
       "         [-2.1895, -1.3615,  1.3255,  0.5632,  0.0529,  0.4835,  1.1977,\n",
       "           0.8252],\n",
       "         [-1.6999, -0.2699, -0.9496, -1.0292,  0.3475, -0.3594, -0.5015,\n",
       "          -0.3627]]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9352e6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.0298, -1.3108, -0.7712,  0.0066],\n",
       "          [ 0.3956,  0.2575,  1.1512, -1.7080]],\n",
       "\n",
       "         [[ 0.9042, -0.2926, -0.6147, -1.3084],\n",
       "          [-0.5278, -0.1160, -1.7883,  1.4322]],\n",
       "\n",
       "         [[-2.1895, -1.3615,  1.3255,  0.5632],\n",
       "          [ 0.0529,  0.4835,  1.1977,  0.8252]],\n",
       "\n",
       "         [[-1.6999, -0.2699, -0.9496, -1.0292],\n",
       "          [ 0.3475, -0.3594, -0.5015, -0.3627]]],\n",
       "\n",
       "\n",
       "        [[[-1.0298, -1.3108, -0.7712,  0.0066],\n",
       "          [ 0.3956,  0.2575,  1.1512, -1.7080]],\n",
       "\n",
       "         [[ 0.9042, -0.2926, -0.6147, -1.3084],\n",
       "          [-0.5278, -0.1160, -1.7883,  1.4322]],\n",
       "\n",
       "         [[-2.1895, -1.3615,  1.3255,  0.5632],\n",
       "          [ 0.0529,  0.4835,  1.1977,  0.8252]],\n",
       "\n",
       "         [[-1.6999, -0.2699, -0.9496, -1.0292],\n",
       "          [ 0.3475, -0.3594, -0.5015, -0.3627]]]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.view( 2, 4, 2, 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "416784a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention( d_in = 8, d_out = 6, context_length=4, dropout=0, num_heads=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4e211748",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_out = mha( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9e903403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0985,  0.0660,  0.2198,  0.0374, -0.1643,  0.0888],\n",
       "         [ 0.2072, -0.1460,  0.2532,  0.0242,  0.4572, -0.3227],\n",
       "         [ 0.2035, -0.1053,  0.2840, -0.1345,  0.2946, -0.2536],\n",
       "         [ 0.1382, -0.1410,  0.2790,  0.0616,  0.2923, -0.1300]],\n",
       "\n",
       "        [[ 0.0985,  0.0660,  0.2198,  0.0374, -0.1643,  0.0888],\n",
       "         [ 0.2072, -0.1460,  0.2532,  0.0242,  0.4572, -0.3227],\n",
       "         [ 0.2035, -0.1053,  0.2840, -0.1345,  0.2946, -0.2536],\n",
       "         [ 0.1382, -0.1410,  0.2790,  0.0616,  0.2923, -0.1300]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f7a58282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 6])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
