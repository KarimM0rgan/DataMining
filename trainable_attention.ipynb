{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "674bd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4\n",
    "output_dimension = 8\n",
    "\n",
    "inputs = torch.nn.Embedding(vocab_size, output_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2026,  0.6488, -0.1370, -1.1637,  0.2137,  0.7658,  1.0017,  0.8374],\n",
       "        [ 0.9614, -1.6557,  1.0424,  1.2569,  0.5858, -1.1519, -0.8093,  1.0280],\n",
       "        [ 1.2628,  0.9452,  0.1253,  0.5801,  1.0466,  0.9004,  0.0573,  0.1743],\n",
       "        [-2.6966, -1.2928,  0.0785,  0.3935,  0.2669, -0.5604, -0.5411,  1.2409]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.weight\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2026,  0.6488, -0.1370, -1.1637,  0.2137,  0.7658,  1.0017,  0.8374],\n",
       "        [ 0.9614, -1.6557,  1.0424,  1.2569,  0.5858, -1.1519, -0.8093,  1.0280],\n",
       "        [ 1.2628,  0.9452,  0.1253,  0.5801,  1.0466,  0.9004,  0.0573,  0.1743],\n",
       "        [-2.6966, -1.2928,  0.0785,  0.3935,  0.2669, -0.5604, -0.5411,  1.2409]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.data # without 'requires_grad=True'\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c3086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dimensions\n",
    "d_in = 8 # inputs.shape[i]\n",
    "d_out = 6 # preferred output size\n",
    "\n",
    "# create weight matrices\n",
    "W_q = torch.nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "W_k = torch.nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "W_v = torch.nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0851,  1.5374,  1.3586,  2.0521,  1.4855,  0.9387],\n",
       "        [ 0.4557, -1.5418,  0.3188, -0.5352, -0.7327,  2.3348],\n",
       "        [ 2.5883,  2.8919,  1.0787,  2.9465,  2.2346,  2.2001],\n",
       "        [-0.9973, -1.9330, -0.2969, -3.5545, -1.6691, -0.5671]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose an input vector and transform it into our query vector using W_q\n",
    "# Note that the output has the preferred size\n",
    "query = inputs @ W_q\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcd2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: tensor([[ 1.5925,  1.7724,  0.5706,  1.6434,  0.9033,  0.3941],\n",
      "        [ 0.5997, -0.8042,  1.0577,  1.7515,  0.8616,  0.8988],\n",
      "        [ 1.8331,  3.2341,  2.8132,  3.5662,  2.3614,  2.5543],\n",
      "        [-0.0117, -2.0766, -2.5004, -2.0864, -2.1412, -1.2068]])\n",
      "Values: tensor([[ 2.4172,  0.6776,  0.2720,  2.0063,  0.8952,  1.5085],\n",
      "        [-0.8929,  0.8504,  1.5221,  0.2792,  1.1410, -0.2061],\n",
      "        [ 3.2835,  3.7520,  1.7643,  1.8684,  1.3785,  3.6897],\n",
      "        [-3.6619, -2.4133, -0.0285, -1.2269,  0.2540, -2.1437]])\n"
     ]
    }
   ],
   "source": [
    "# calculate attention scores using the keys generated by W_k:\n",
    "keys = inputs @ W_k\n",
    "values = inputs @ W_v\n",
    "print(\"Keys:\", keys)\n",
    "print(\"Values:\", values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 17.5908,   9.4310,  38.5359, -22.3202])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention scores are how important a token is. Ex. when translating word by word,\n",
    "# the word being translated gets the highest att score. the others get scores too but not as big as the meant word\n",
    "attention_scores = query @ keys.T\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1123b",
   "metadata": {},
   "source": [
    "Attention weights are different as they refer to the input and other inputs with respect to the highest attention (query) score input\n",
    "Ex. a23 --> a is the name of the weight, 2 is the highest score, and 3 is the position of the input with respect to the highest, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e2383b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9335e-04, 6.9123e-06, 9.9980e-01, 1.6223e-11])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dividing by the root of shape of a row in keys to keep them in a reasonable range\n",
    "attention_weights = torch.softmax( attention_scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50eab267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ea06dee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.2833, 3.7514, 1.7641, 1.8684, 1.3784, 3.6893])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector = attention_weights @ values\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a first version of a SimpleAttention class:\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    # create weight matrices:\n",
    "    self.W_q = nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "    self.W_k = nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "    self.W_v = nn.Parameter( torch.rand( d_in, d_out ), requires_grad=False )\n",
    "\n",
    "  # x = embedding vectors (inputs)\n",
    "  def forward( self, x ):\n",
    "    queries = x @ self.W_q\n",
    "    keys = x @ self.W_k\n",
    "    values = x @ self.W_v\n",
    "\n",
    "    scores = queries @ keys.T\n",
    "    weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "    context = weights @ values\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7ca78c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how to use this class:\n",
    "# instantiate an instance of it:\n",
    "simple = SimpleAttention( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee10547a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.3949, 0.1490, 0.5330, 0.4385, 0.1507, 0.6974],\n",
       "        [0.0810, 0.6689, 0.7093, 0.0489, 0.9008, 0.6195],\n",
       "        [0.2792, 0.3700, 0.3167, 0.5520, 0.5143, 0.1974],\n",
       "        [0.2238, 0.9672, 0.4168, 0.9716, 0.1322, 0.2362],\n",
       "        [0.3618, 0.5748, 0.1897, 0.6266, 0.5694, 0.9321],\n",
       "        [0.5822, 0.4235, 0.4769, 0.4325, 0.1058, 0.6787],\n",
       "        [0.1098, 0.5989, 0.8953, 0.7697, 0.6388, 0.4676],\n",
       "        [0.4718, 0.9047, 0.1847, 0.8678, 0.2331, 0.6411]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "efbbf836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5701,  2.3257,  1.9721,  2.3300,  1.6557,  2.9421],\n",
       "        [ 1.7132,  2.5713,  2.3164,  2.4417,  1.9381,  3.3222],\n",
       "        [ 1.7311,  2.6024,  2.3361,  2.4728,  1.9512,  3.3526],\n",
       "        [-0.7629, -0.1420, -2.6364, -0.2343, -1.4419, -2.1611]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0e180fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a second version of a SimpleAttention class ;\n",
    "# it uses nn.Linear to do things more efficiently and gives better training results\n",
    "\n",
    "class SimpleAttentionV2( nn.Module ):\n",
    "  def __init__(self, d_in, d_out):\n",
    "    super().__init__()\n",
    "    # create weight matrices:\n",
    "    self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "\n",
    "  # x = embedding vectors (inputs)\n",
    "  def forward( self, x ):\n",
    "    queries = self.W_q( x )\n",
    "    keys = self.W_k( x )\n",
    "    values = self.W_v( x )\n",
    "\n",
    "    scores = queries @ keys.T\n",
    "    weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "    context = weights @ values\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6ce88c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's how to use this class:\n",
    "# instantiate an instance of it:\n",
    "simple = SimpleAttentionV2( d_in = 8, d_out = 6 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a21b8187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1999, -0.0027,  0.3246,  0.2052,  0.2975, -0.1162],\n",
       "        [ 0.0551,  0.0104,  0.1952,  0.0808,  0.2693, -0.1608],\n",
       "        [ 0.1449,  0.0077,  0.2736,  0.1402,  0.2850, -0.1151],\n",
       "        [ 0.0479,  0.0077,  0.1920,  0.0908,  0.2710, -0.1757]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vectors = simple( inputs )\n",
    "context_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456ebf2",
   "metadata": {},
   "source": [
    "- The problem with this is that each context vector uses information from ALL of the embedding vectors\n",
    "- In practice, we should only use information about the preceding embedding vectors\n",
    "- To accomplish this, we'll implement causal attention AKA masked attention\n",
    "- It briefly means hiding future words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c14c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2021, 0.2944, 0.2535, 0.2500],\n",
       "        [0.3036, 0.2168, 0.2889, 0.1907],\n",
       "        [0.2446, 0.2528, 0.2562, 0.2464],\n",
       "        [0.3020, 0.2217, 0.3018, 0.1745]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a hack to get some example weights to work with!\n",
    "# weights = simple( inputs ) # The hack didn't work :(\n",
    "\n",
    "queries = simple.W_q(inputs)\n",
    "keys = simple.W_k(inputs)\n",
    "values = simple.W_v(inputs)\n",
    "\n",
    "scores = queries @ keys.T\n",
    "weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a3d61868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that these have already been normalized:\n",
    "weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "61dc47e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking method #1\n",
    "simple_mask = torch.tril( torch.ones( weights.shape[0], weights.shape[0] ) ) #Triangular mask\n",
    "simple_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9f55eb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2021, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3036, 0.2168, 0.0000, 0.0000],\n",
       "        [0.2446, 0.2528, 0.2562, 0.0000],\n",
       "        [0.3020, 0.2217, 0.3018, 0.1745]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = weights * simple_mask\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d8598a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2021, 0.5204, 0.7536, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights.sum( dim=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e15ffec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2021],\n",
       "        [0.5204],\n",
       "        [0.7536],\n",
       "        [1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, we need to normalize the masked_weights so that each row has sum 1 as it is good for optimization\n",
    "# What this code does -> simple_mask / row_sums\n",
    "row_sums = masked_weights.sum( dim=-1, keepdim=True)\n",
    "row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ae83a25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights = masked_weights / row_sums\n",
    "masked_weights.sum( dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "94afa496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5834, 0.4166, 0.0000, 0.0000],\n",
       "        [0.3245, 0.3355, 0.3400, 0.0000],\n",
       "        [0.3020, 0.2217, 0.3018, 0.1745]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bccd4621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking method #2\n",
    "# This way scores -> mask -> soft max\n",
    "mask = torch.triu( torch.ones(weights.shape[0], weights.shape[0]), diagonal = 1 ) \n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "baf74a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e85769cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2021, 0.2944, 0.2535, 0.2500],\n",
       "        [0.3036, 0.2168, 0.2889, 0.1907],\n",
       "        [0.2446, 0.2528, 0.2562, 0.2464],\n",
       "        [0.3020, 0.2217, 0.3018, 0.1745]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abd0fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2021,   -inf,   -inf,   -inf],\n",
       "        [0.3036, 0.2168,   -inf,   -inf],\n",
       "        [0.2446, 0.2528, 0.2562,   -inf],\n",
       "        [0.3020, 0.2217, 0.3018, 0.1745]], grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We masked the values first by hiding future values with -infinity\n",
    "weights = weights.masked_fill( mask.bool(), -torch.inf )\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fe938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5217, 0.4783, 0.0000, 0.0000],\n",
       "        [0.3311, 0.3339, 0.3350, 0.0000],\n",
       "        [0.2630, 0.2427, 0.2629, 0.2315]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, every row sums up to 1\n",
    "masked_weights = torch.softmax( weights, dim=-1 )\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropout Mask\n",
    "# idea: randomly select some data to leave out to avoid overfitting\n",
    "dropout = nn.Dropout( 0.5 ) # 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "068c12b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 2., 2., 2., 0.],\n",
       "        [2., 0., 2., 0., 2., 2.],\n",
       "        [2., 2., 2., 0., 2., 2.],\n",
       "        [2., 0., 0., 2., 0., 0.],\n",
       "        [2., 2., 0., 0., 2., 2.],\n",
       "        [2., 0., 2., 0., 2., 2.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout( torch.ones(6,6) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a52983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to be able to give our LLM batches of input\n",
    "# for example:\n",
    "batches = torch.stack( (inputs, inputs), dim=0) #stack 2 inputs on top of eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5b05a630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2026,  0.6488, -0.1370, -1.1637,  0.2137,  0.7658,  1.0017,\n",
       "           0.8374],\n",
       "         [ 0.9614, -1.6557,  1.0424,  1.2569,  0.5858, -1.1519, -0.8093,\n",
       "           1.0280],\n",
       "         [ 1.2628,  0.9452,  0.1253,  0.5801,  1.0466,  0.9004,  0.0573,\n",
       "           0.1743],\n",
       "         [-2.6966, -1.2928,  0.0785,  0.3935,  0.2669, -0.5604, -0.5411,\n",
       "           1.2409]],\n",
       "\n",
       "        [[ 0.2026,  0.6488, -0.1370, -1.1637,  0.2137,  0.7658,  1.0017,\n",
       "           0.8374],\n",
       "         [ 0.9614, -1.6557,  1.0424,  1.2569,  0.5858, -1.1519, -0.8093,\n",
       "           1.0280],\n",
       "         [ 1.2628,  0.9452,  0.1253,  0.5801,  1.0466,  0.9004,  0.0573,\n",
       "           0.1743],\n",
       "         [-2.6966, -1.2928,  0.0785,  0.3935,  0.2669, -0.5604, -0.5411,\n",
       "           1.2409]]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8667e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches.shape\n",
    "# The output means 2 inputs, each tensor has 4 tokens, and each token is 8-dim vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0f2af53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class needs to handle batches of input!\n",
    "\n",
    "class CausalAttention( nn.Module ):\n",
    "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "    super().__init__()\n",
    "    self.d_out = d_out\n",
    "    # create weight matrices:\n",
    "    self.W_q = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_k = nn.Linear( d_in, d_out, bias=False )\n",
    "    self.W_v = nn.Linear( d_in, d_out, bias=False )\n",
    "    \n",
    "    # include dropout:\n",
    "    self.dropout = nn.Dropout( dropout )\n",
    "    \n",
    "    # use the following to manage memory efficiently\n",
    "    # When passing this to a GPU, it's better because GPUs dont read tensors\n",
    "    self.register_buffer(\n",
    "        'mask',\n",
    "        torch.triu( torch.ones(context_length, context_length), diagonal = 1 )\n",
    "    )\n",
    "\n",
    "  # x = embedding vectors (inputs)\n",
    "  def forward( self, x ):\n",
    "    b, num_tokens, d_in = x.shape # b is batch size (num inputs)\n",
    "    queries = self.W_q( x )\n",
    "    keys = self.W_k( x )\n",
    "    values = self.W_v( x )\n",
    "\n",
    "    scores = queries @ keys.transpose(1,2)\n",
    "    scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # inplace operation for better efficiency\n",
    "    weights = torch.softmax( scores / keys.shape[-1]**0.5, dim = -1 )\n",
    "    weights = self.dropout( weights )\n",
    "    \n",
    "    context = weights @ values\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "61e92040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a causal attention mechanism:\n",
    "causal = CausalAttention( d_in=8, d_out=6, context_length=4, dropout=0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d910ad73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4950, -0.2449,  0.1706, -0.5732,  0.5883, -0.2651],\n",
       "         [-0.5732,  0.2960, -0.0670, -0.2901,  0.6681, -0.1605],\n",
       "         [-0.4589,  0.3969, -0.0690, -0.2954,  0.4442, -0.0536],\n",
       "         [-0.5587,  0.3625, -0.0361, -0.1037,  0.2258, -0.0034]],\n",
       "\n",
       "        [[ 0.4950, -0.2449,  0.1706, -0.5732,  0.5883, -0.2651],\n",
       "         [-0.5732,  0.2960, -0.0670, -0.2901,  0.6681, -0.1605],\n",
       "         [-0.4589,  0.3969, -0.0690, -0.2954,  0.4442, -0.0536],\n",
       "         [-0.5587,  0.3625, -0.0361, -0.1037,  0.2258, -0.0034]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal( batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "88fd5462",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Parameter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# everything below is just to show what happens with batches\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mW_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m queries\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Parameter' object is not callable"
     ]
    }
   ],
   "source": [
    "# everything below is just to show what happens with batches\n",
    "\n",
    "queries = W_q( batches )\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0dada1fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Parameter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mW_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m keys\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Parameter' object is not callable"
     ]
    }
   ],
   "source": [
    "keys = W_k( batches )\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f97bcea3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "keys.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ecd368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336d121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcdedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed92953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67064b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fb817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422c9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
