{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f3f929",
   "metadata": {},
   "source": [
    "Import PyTorch modules needed for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a105321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e141d",
   "metadata": {},
   "source": [
    "Define parameters: batch size (b), sequence length (num_tokens), input dimension (d_in), output dimension (d_out), number of heads (num_heads), and head dimension (head_dim = d_out // num_heads). Initialize dropout as an nn.Dropout module with the specified rate (0.0 here, meaning no dropout applied, but the module is ready for non-zero rates). \n",
    " \n",
    "Efficiency: Defining once avoids recreating per batch, and dropout doesn’t add matrix multiplications, preserving the efficiency of the batched attention mechanism (one multiplication for scores, one for weighted sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8accda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input shape\n",
    "b = 2  # Batch size\n",
    "num_tokens = 3  # Sequence length (context length)\n",
    "d_in = 4  # Input embedding dimension\n",
    "\n",
    "# Output shape\n",
    "d_out = 6  # Output dimension (must be divisible by num_heads)\n",
    "num_heads = 3  # Number of attention heads\n",
    "head_dim = d_out // num_heads  # Per-head dimension (2 here)\n",
    "\n",
    "dropout = nn.Dropout(0)  # Dropout rate (0 for no dropout)\n",
    "qkv_bias = False  # No bias in projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58be94f",
   "metadata": {},
   "source": [
    "Generate a random input tensor x with shape (b, num_tokens, d_in), representing batched token embeddings. All computations start from x; its columns (d_in) must match the input size of projection layers for matrix multiplication. \n",
    "\n",
    "Efficiency: Random data allows testing without real datasets, and small sizes (like 3 tokens) keep computations fast while demonstrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x shape: torch.Size([2, 3, 4])\n",
      "Input x:\n",
      " tensor([[[ 0.5846, -2.4947,  1.4039,  1.6177],\n",
      "         [ 1.0283,  0.4905, -0.9044,  1.4440],\n",
      "         [ 0.2894, -0.3367, -1.3002,  1.1622]],\n",
      "\n",
      "        [[ 1.5172,  0.1696, -0.0997, -0.7608],\n",
      "         [-0.5116,  1.0167,  0.2778, -0.4208],\n",
      "         [-0.5266,  1.3310, -0.2707,  0.5006]]])\n"
     ]
    }
   ],
   "source": [
    "# Sample input: batched random embeddings\n",
    "x = torch.randn(b, num_tokens, d_in)\n",
    "print(\"Input x shape:\", x.shape)\n",
    "print(\"Input x:\\n\", x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0085ef6",
   "metadata": {},
   "source": [
    "Create linear projection variables for queries (W_query), keys (W_key), values (W_value), and output (out_proj). Also create the causal mask as an upper-triangular tensor. Projections transform x to attention spaces, and the mask prevents attending to future tokens. Projections change dimensions (from d_in to d_out)\n",
    "\n",
    "Efficiency: Shared projections (one per Q/K/V) are cheaper than per-head ones; mask is computed once, avoiding recreation per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314d0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection layers\n",
    "W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "out_proj = nn.Linear(d_out, d_out)  # For combining heads later\n",
    "\n",
    "# Causal mask (upper triangular, 1s above diagonal)\n",
    "# We won't use register buffer because this won't be transferred to another gpu\n",
    "mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b43b1",
   "metadata": {},
   "source": [
    "Apply the linear projections to x to get keys, queries, and values, each with shape (b, num_tokens, d_out). This transforms the input into specialized representations for attention (queries search, keys are searched, values are aggregated). The full d_out is concatenated across heads. For matrix multiplication in projections: x's columns (d_in) match W's rows. \n",
    "\n",
    "Efficiency: Only 3 multiplications here (one per projection) vs. naive per-head (3 * num_heads=9); shared across heads reduces parameters/compute in large LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da47cca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys shape: torch.Size([2, 3, 6])\n",
      "Keys:\n",
      " tensor([[[-1.3128, -1.1525, -0.0044,  0.3529, -1.0175, -0.4947],\n",
      "         [-0.0478,  0.2416,  0.3827, -0.2477,  0.8610, -0.1500],\n",
      "         [ 0.3607,  0.4046,  0.1977, -0.3166,  0.5330, -0.0159]],\n",
      "\n",
      "        [[-0.3900,  0.0901,  0.8316,  0.0871, -0.3479, -0.0886],\n",
      "         [ 0.1938,  0.0591, -0.2975,  0.0132,  0.3347,  0.1114],\n",
      "         [ 0.3892,  0.2314, -0.3749, -0.1714,  0.9555,  0.0942]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Project input to keys, queries, values\n",
    "keys = W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "print(\"Keys shape:\", keys.shape)\n",
    "print(\"Keys:\\n\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([2, 3, 6])\n",
      "Queries:\n",
      " tensor([[[-0.7678, -1.3186, -0.2261, -0.1504, -1.0803, -0.5805],\n",
      "         [ 0.9581,  0.6228, -0.7427, -0.2901,  0.8595, -1.0349],\n",
      "         [ 0.3950,  0.3857, -0.2860,  0.1255,  0.3254, -0.5456]],\n",
      "\n",
      "        [[ 0.6956,  1.0206, -0.2647,  0.7459,  0.4569, -0.4890],\n",
      "         [ 0.0291, -0.0745,  0.0721, -0.4411,  0.2085,  0.3205],\n",
      "         [ 0.3327, -0.0129, -0.2063, -0.8172,  0.5573, -0.0188]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = W_query(x)\n",
    "print(\"Queries shape:\", queries.shape)\n",
    "print(\"Queries:\\n\", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values shape: torch.Size([2, 3, 6])\n",
      "Values:\n",
      " tensor([[[ 0.5184, -0.1331, -0.0145,  0.2668, -1.0190, -0.0328],\n",
      "         [ 0.0262,  1.5439, -0.2288, -0.3577, -0.6426,  1.0325],\n",
      "         [ 0.1131,  0.8719, -0.1355, -0.1839, -0.3956,  0.4536]],\n",
      "\n",
      "        [[-0.9401,  0.3831, -0.6437,  0.0657, -0.0758,  0.5278],\n",
      "         [ 0.1385, -0.1387,  0.2486, -0.0979,  0.3257, -0.1112],\n",
      "         [ 0.4374,  0.5414,  0.3435, -0.3193,  0.0749,  0.2452]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "values = W_value(x)\n",
    "print(\"Values shape:\", values.shape)\n",
    "print(\"Values:\\n\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eda2cb",
   "metadata": {},
   "source": [
    "Use .view() to reshape keys, values, and queries by splitting the last dimension (d_out) into num_heads and head_dim, resulting in shape (b, num_tokens, num_heads, head_dim). This divides the projected tensors into separate heads without copying data (cheap operation). Allows each head to compute attention independently in its own low-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ebc142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys after view shape: torch.Size([2, 3, 3, 2])\n",
      "Keys after view:\n",
      " tensor([[[[-1.3128, -1.1525],\n",
      "          [-0.0044,  0.3529],\n",
      "          [-1.0175, -0.4947]],\n",
      "\n",
      "         [[-0.0478,  0.2416],\n",
      "          [ 0.3827, -0.2477],\n",
      "          [ 0.8610, -0.1500]],\n",
      "\n",
      "         [[ 0.3607,  0.4046],\n",
      "          [ 0.1977, -0.3166],\n",
      "          [ 0.5330, -0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.3900,  0.0901],\n",
      "          [ 0.8316,  0.0871],\n",
      "          [-0.3479, -0.0886]],\n",
      "\n",
      "         [[ 0.1938,  0.0591],\n",
      "          [-0.2975,  0.0132],\n",
      "          [ 0.3347,  0.1114]],\n",
      "\n",
      "         [[ 0.3892,  0.2314],\n",
      "          [-0.3749, -0.1714],\n",
      "          [ 0.9555,  0.0942]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We implicitly split the matrix by adding a `num_heads` dimension\n",
    "# Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        \n",
    "# Reshape to separate heads (add num_heads dimension)\n",
    "keys = keys.view(b, num_tokens, num_heads, head_dim)\n",
    "print(\"Keys after view shape:\", keys.shape)\n",
    "print(\"Keys after view:\\n\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba6e0b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values after view shape: torch.Size([2, 3, 3, 2])\n",
      "Values after view:\n",
      " tensor([[[[ 0.5184, -0.1331],\n",
      "          [-0.0145,  0.2668],\n",
      "          [-1.0190, -0.0328]],\n",
      "\n",
      "         [[ 0.0262,  1.5439],\n",
      "          [-0.2288, -0.3577],\n",
      "          [-0.6426,  1.0325]],\n",
      "\n",
      "         [[ 0.1131,  0.8719],\n",
      "          [-0.1355, -0.1839],\n",
      "          [-0.3956,  0.4536]]],\n",
      "\n",
      "\n",
      "        [[[-0.9401,  0.3831],\n",
      "          [-0.6437,  0.0657],\n",
      "          [-0.0758,  0.5278]],\n",
      "\n",
      "         [[ 0.1385, -0.1387],\n",
      "          [ 0.2486, -0.0979],\n",
      "          [ 0.3257, -0.1112]],\n",
      "\n",
      "         [[ 0.4374,  0.5414],\n",
      "          [ 0.3435, -0.3193],\n",
      "          [ 0.0749,  0.2452]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "values = values.view(b, num_tokens, num_heads, head_dim)\n",
    "print(\"Values after view shape:\", values.shape)\n",
    "print(\"Values after view:\\n\", values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f545a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries after view shape: torch.Size([2, 3, 3, 2])\n",
      "Queries after view:\n",
      " tensor([[[[-0.7678, -1.3186],\n",
      "          [-0.2261, -0.1504],\n",
      "          [-1.0803, -0.5805]],\n",
      "\n",
      "         [[ 0.9581,  0.6228],\n",
      "          [-0.7427, -0.2901],\n",
      "          [ 0.8595, -1.0349]],\n",
      "\n",
      "         [[ 0.3950,  0.3857],\n",
      "          [-0.2860,  0.1255],\n",
      "          [ 0.3254, -0.5456]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6956,  1.0206],\n",
      "          [-0.2647,  0.7459],\n",
      "          [ 0.4569, -0.4890]],\n",
      "\n",
      "         [[ 0.0291, -0.0745],\n",
      "          [ 0.0721, -0.4411],\n",
      "          [ 0.2085,  0.3205]],\n",
      "\n",
      "         [[ 0.3327, -0.0129],\n",
      "          [-0.2063, -0.8172],\n",
      "          [ 0.5573, -0.0188]]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = queries.view(b, num_tokens, num_heads, head_dim)\n",
    "print(\"Queries after view shape:\", queries.shape)\n",
    "print(\"Queries after view:\\n\", queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3df5af",
   "metadata": {},
   "source": [
    "Transpose the sequence (num_tokens) and heads dimensions to get shape (b, num_heads, num_tokens, head_dim). This positions num_heads as a batch-like dimension, allowing PyTorch to compute operations in parallel across heads without loops. Prepares for efficient matrix multiplication (in attention scores, queries' last dim head_dim will match keys.T's rows). \n",
    "\n",
    "Efficiency: Transpose is fast; it enables one vectorized multiplication later instead of num_heads separate ones—vital for LLMs to minimize calculations on huge matrices. Basically changing parameters positions to make it suitable for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99df6146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys after transpose shape: torch.Size([2, 3, 3, 2])\n",
      "Keys after transpose:\n",
      " tensor([[[[-1.3128, -1.1525],\n",
      "          [-0.0478,  0.2416],\n",
      "          [ 0.3607,  0.4046]],\n",
      "\n",
      "         [[-0.0044,  0.3529],\n",
      "          [ 0.3827, -0.2477],\n",
      "          [ 0.1977, -0.3166]],\n",
      "\n",
      "         [[-1.0175, -0.4947],\n",
      "          [ 0.8610, -0.1500],\n",
      "          [ 0.5330, -0.0159]]],\n",
      "\n",
      "\n",
      "        [[[-0.3900,  0.0901],\n",
      "          [ 0.1938,  0.0591],\n",
      "          [ 0.3892,  0.2314]],\n",
      "\n",
      "         [[ 0.8316,  0.0871],\n",
      "          [-0.2975,  0.0132],\n",
      "          [-0.3749, -0.1714]],\n",
      "\n",
      "         [[-0.3479, -0.0886],\n",
      "          [ 0.3347,  0.1114],\n",
      "          [ 0.9555,  0.0942]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "# Transpose to prepare for batched matrix multiplication (heads as batch dim)\n",
    "keys = keys.transpose(1, 2)\n",
    "print(\"Keys after transpose shape:\", keys.shape)\n",
    "print(\"Keys after transpose:\\n\", keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2677b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries after transpose shape: torch.Size([2, 3, 3, 2])\n",
      "Queries after transpose:\n",
      " tensor([[[[-0.7678, -1.3186],\n",
      "          [-0.2261, -0.1504],\n",
      "          [-1.0803, -0.5805]],\n",
      "\n",
      "         [[ 0.9581,  0.6228],\n",
      "          [-0.7427, -0.2901],\n",
      "          [ 0.8595, -1.0349]],\n",
      "\n",
      "         [[ 0.3950,  0.3857],\n",
      "          [-0.2860,  0.1255],\n",
      "          [ 0.3254, -0.5456]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6956,  1.0206],\n",
      "          [-0.2647,  0.7459],\n",
      "          [ 0.4569, -0.4890]],\n",
      "\n",
      "         [[ 0.0291, -0.0745],\n",
      "          [ 0.0721, -0.4411],\n",
      "          [ 0.2085,  0.3205]],\n",
      "\n",
      "         [[ 0.3327, -0.0129],\n",
      "          [-0.2063, -0.8172],\n",
      "          [ 0.5573, -0.0188]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = queries.transpose(1, 2)\n",
    "print(\"Queries after transpose shape:\", queries.shape)\n",
    "print(\"Queries after transpose:\\n\", queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd63450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values after transpose shape: torch.Size([2, 3, 3, 2])\n",
      "Values after transpose:\n",
      " tensor([[[[ 0.5184, -0.1331],\n",
      "          [ 0.0262,  1.5439],\n",
      "          [ 0.1131,  0.8719]],\n",
      "\n",
      "         [[-0.0145,  0.2668],\n",
      "          [-0.2288, -0.3577],\n",
      "          [-0.1355, -0.1839]],\n",
      "\n",
      "         [[-1.0190, -0.0328],\n",
      "          [-0.6426,  1.0325],\n",
      "          [-0.3956,  0.4536]]],\n",
      "\n",
      "\n",
      "        [[[-0.9401,  0.3831],\n",
      "          [ 0.1385, -0.1387],\n",
      "          [ 0.4374,  0.5414]],\n",
      "\n",
      "         [[-0.6437,  0.0657],\n",
      "          [ 0.2486, -0.0979],\n",
      "          [ 0.3435, -0.3193]],\n",
      "\n",
      "         [[-0.0758,  0.5278],\n",
      "          [ 0.3257, -0.1112],\n",
      "          [ 0.0749,  0.2452]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "values = values.transpose(1, 2)\n",
    "print(\"Values after transpose shape:\", values.shape)\n",
    "print(\"Values after transpose:\\n\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c621cf5",
   "metadata": {},
   "source": [
    "Calculate attn_scores as the scaled dot-product between queries and transposed keys: queries @ keys.transpose(2, 3). Shape: (b, num_heads, num_tokens, num_tokens). This measures similarity between tokens per head. The transpose(2,3) flips keys' last two dims to get K^T for dot-product. \n",
    "\n",
    "Efficiency: Thanks to prior reshape/transpose, this is one batched multiplication across all heads—no loops. Naive approach: num_heads multiplications (inefficient for LLMs with 32+ heads, as extra calls slow down on large matrices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20de4bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape: torch.Size([2, 3, 3, 3])\n",
      "Attention scores:\n",
      " tensor([[[[ 2.5277e+00, -2.8185e-01, -8.1044e-01],\n",
      "          [ 4.7021e-01, -2.5526e-02, -1.4241e-01],\n",
      "          [ 2.0873e+00, -8.8567e-02, -6.2450e-01]],\n",
      "\n",
      "         [[ 2.1561e-01,  2.1245e-01, -7.7876e-03],\n",
      "          [-9.9141e-02, -2.1240e-01, -5.4949e-02],\n",
      "          [-3.6900e-01,  5.8528e-01,  4.9750e-01]],\n",
      "\n",
      "         [[-5.9269e-01,  2.8222e-01,  2.0436e-01],\n",
      "          [ 2.2891e-01, -2.6508e-01, -1.5443e-01],\n",
      "          [-6.1238e-02,  3.6205e-01,  1.8214e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.7929e-01,  1.9518e-01,  5.0693e-01],\n",
      "          [ 1.7044e-01, -7.2047e-03,  6.9587e-02],\n",
      "          [-2.2228e-01,  5.9661e-02,  6.4687e-02]],\n",
      "\n",
      "         [[ 1.7685e-02, -9.6325e-03,  1.8700e-03],\n",
      "          [ 2.1516e-02, -2.7273e-02,  4.8576e-02],\n",
      "          [ 2.0132e-01, -5.7797e-02, -1.3308e-01]],\n",
      "\n",
      "         [[-1.1459e-01,  1.0990e-01,  3.1666e-01],\n",
      "          [ 1.4421e-01, -1.6009e-01, -2.7410e-01],\n",
      "          [-1.9220e-01,  1.8442e-01,  5.3070e-01]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "# Compute raw attention scores (dot-product per head)\n",
    "attn_scores = queries @ keys.transpose(2, 3)  # Batched matmul\n",
    "print(\"Attention scores shape:\", attn_scores.shape)\n",
    "print(\"Attention scores:\\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36e5a5",
   "metadata": {},
   "source": [
    "Apply the mask to current sequence length, convert to boolean, and fill masked positions in attn_scores with -inf. This prevents tokens from attending to future ones. Without masking, the model would \"cheat\" by seeing ahead; -inf ensures softmax ignores them. \n",
    "\n",
    "Efficiency: In-place fill (masked_fill_) is memory-efficient; precomputed mask avoids recalculating per batch, saving time in LLMs with repeated forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa0de8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask shape: torch.Size([3, 3])\n",
      "Mask:\n",
      " tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# Original mask truncated to the number of tokens and converted to boolean\n",
    "# Apply causal mask (truncate and convert to bool)\n",
    "mask_bool = mask.bool()[:num_tokens, :num_tokens]\n",
    "print(\"Mask shape:\", mask_bool.shape)\n",
    "print(\"Mask:\\n\", mask_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68d4d325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention scores shape: torch.Size([2, 3, 3, 3])\n",
      "Masked attention scores:\n",
      " tensor([[[[ 2.5277,    -inf,    -inf],\n",
      "          [ 0.4702, -0.0255,    -inf],\n",
      "          [ 2.0873, -0.0886, -0.6245]],\n",
      "\n",
      "         [[ 0.2156,    -inf,    -inf],\n",
      "          [-0.0991, -0.2124,    -inf],\n",
      "          [-0.3690,  0.5853,  0.4975]],\n",
      "\n",
      "         [[-0.5927,    -inf,    -inf],\n",
      "          [ 0.2289, -0.2651,    -inf],\n",
      "          [-0.0612,  0.3620,  0.1821]]],\n",
      "\n",
      "\n",
      "        [[[-0.1793,    -inf,    -inf],\n",
      "          [ 0.1704, -0.0072,    -inf],\n",
      "          [-0.2223,  0.0597,  0.0647]],\n",
      "\n",
      "         [[ 0.0177,    -inf,    -inf],\n",
      "          [ 0.0215, -0.0273,    -inf],\n",
      "          [ 0.2013, -0.0578, -0.1331]],\n",
      "\n",
      "         [[-0.1146,    -inf,    -inf],\n",
      "          [ 0.1442, -0.1601,    -inf],\n",
      "          [-0.1922,  0.1844,  0.5307]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Fill scores with -inf where masked\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "print(\"Masked attention scores shape:\", attn_scores.shape)\n",
    "print(\"Masked attention scores:\\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c601e45d",
   "metadata": {},
   "source": [
    "Calc attn_weights by attn_scores / sqrt(keys.shape) and apply softmax along the last dimension to convert scores to weights, and apply the pre-initialized dropout module. Softmax normalizes scores into attention weights (summing to 1 per row); dropout (at 0.0) regularizes by randomly zeroing elements during training. \n",
    "\n",
    "Efficiency: Using the pre-initialized dropout avoids recreating the module, unlike the naive approach of looping over heads (which would repeat operations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa0ba14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights after softmax shape: torch.Size([2, 3, 3, 3])\n",
      "Attention weights after softmax:\n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5867, 0.4133, 0.0000],\n",
      "          [0.7344, 0.1577, 0.1079]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5200, 0.4800, 0.0000],\n",
      "          [0.2079, 0.4083, 0.3837]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5864, 0.4136, 0.0000],\n",
      "          [0.2827, 0.3814, 0.3358]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5314, 0.4686, 0.0000],\n",
      "          [0.2902, 0.3543, 0.3555]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5086, 0.4914, 0.0000],\n",
      "          [0.3814, 0.3175, 0.3011]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5536, 0.4464, 0.0000],\n",
      "          [0.2517, 0.3286, 0.4197]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute attention weights (scaled softmax)\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Attention weights after softmax shape:\", attn_weights.shape)\n",
    "print(\"Attention weights after softmax:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab76bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropout shape: torch.Size([2, 3, 3, 3])\n",
      "After dropout:\n",
      " tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5867, 0.4133, 0.0000],\n",
      "          [0.7344, 0.1577, 0.1079]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5200, 0.4800, 0.0000],\n",
      "          [0.2079, 0.4083, 0.3837]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5864, 0.4136, 0.0000],\n",
      "          [0.2827, 0.3814, 0.3358]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5314, 0.4686, 0.0000],\n",
      "          [0.2902, 0.3543, 0.3555]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5086, 0.4914, 0.0000],\n",
      "          [0.3814, 0.3175, 0.3011]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5536, 0.4464, 0.0000],\n",
      "          [0.2517, 0.3286, 0.4197]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply dropout using pre-initialized module (rate=0.0)\n",
    "attn_weights = dropout(attn_weights)\n",
    "print(\"After dropout shape:\", attn_weights.shape)\n",
    "print(\"After dropout:\\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8146312",
   "metadata": {},
   "source": [
    "attn_weights @ values, then transpose back to (b, num_tokens, num_heads, head_dim) to aggregate relevant values based on attention weights per head; transpose prepares for combining heads. \n",
    "\n",
    "Efficiency: One batched multiplication here; naive: num_heads muls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc23c437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vec after matrix multiplication and transpose shape: torch.Size([2, 3, 3, 2])\n",
      "Context vec after matrix multiplication and transpose:\n",
      " tensor([[[[ 0.5184, -0.1331],\n",
      "          [-0.0145,  0.2668],\n",
      "          [-1.0190, -0.0328]],\n",
      "\n",
      "         [[ 0.3150,  0.5600],\n",
      "          [-0.1174, -0.0329],\n",
      "          [-0.8633,  0.4078]],\n",
      "\n",
      "         [[ 0.3971,  0.2398],\n",
      "          [-0.1484, -0.1611],\n",
      "          [-0.6661,  0.5369]]],\n",
      "\n",
      "\n",
      "        [[[-0.9401,  0.3831],\n",
      "          [-0.6437,  0.0657],\n",
      "          [-0.0758,  0.5278]],\n",
      "\n",
      "         [[-0.4346,  0.1385],\n",
      "          [-0.2052, -0.0147],\n",
      "          [ 0.1035,  0.2425]],\n",
      "\n",
      "         [[-0.0683,  0.2545],\n",
      "          [-0.0631, -0.1022],\n",
      "          [ 0.1194,  0.1993]]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Shape: (b, num_tokens, num_heads, head_dim)\n",
    "# Weighted sum of values and transpose back\n",
    "context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "print(\"Context vec after matrix multiplication and transpose shape:\", context_vec.shape)\n",
    "print(\"Context vec after matrix multiplication and transpose:\\n\", context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ac372",
   "metadata": {},
   "source": [
    "We used .contigous here to recombine (concatenate) all heads, then .view() to flatten heads back to d_out (shape (b, num_tokens, d_out)), and apply out_proj; projection mixes it for better expressivity. \n",
    "\n",
    "Efficiency: Contiguous+view is fast (no mul); one final multiplication in projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c43aa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vec after view shape: torch.Size([2, 3, 6])\n",
      "Context vec after view:\n",
      " tensor([[[ 0.5184, -0.1331, -0.0145,  0.2668, -1.0190, -0.0328],\n",
      "         [ 0.3150,  0.5600, -0.1174, -0.0329, -0.8633,  0.4078],\n",
      "         [ 0.3971,  0.2398, -0.1484, -0.1611, -0.6661,  0.5369]],\n",
      "\n",
      "        [[-0.9401,  0.3831, -0.6437,  0.0657, -0.0758,  0.5278],\n",
      "         [-0.4346,  0.1385, -0.2052, -0.0147,  0.1035,  0.2425],\n",
      "         [-0.0683,  0.2545, -0.0631, -0.1022,  0.1194,  0.1993]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "# Combine heads (flatten back to d_out)\n",
    "context_vec = context_vec.contiguous().view(b, num_tokens, d_out)\n",
    "print(\"Context vec after view shape:\", context_vec.shape)\n",
    "print(\"Context vec after view:\\n\", context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdffe0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output shape: torch.Size([2, 3, 6])\n",
      "Final output:\n",
      " tensor([[[ 0.3552,  0.2756, -0.2894,  0.2334, -0.6752, -0.3562],\n",
      "         [ 0.4371,  0.2421, -0.0093,  0.6206, -0.9919, -0.2705],\n",
      "         [ 0.4519,  0.0296,  0.0292,  0.5685, -0.9382, -0.3079]],\n",
      "\n",
      "        [[ 0.9782,  0.5996,  0.2588,  0.4722, -0.5337, -0.0014],\n",
      "         [ 0.6233,  0.2310,  0.1319,  0.2327, -0.3441,  0.0383],\n",
      "         [ 0.4272,  0.0567,  0.1532,  0.2812, -0.4479,  0.0405]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Optional output projection\n",
    "context_vec = out_proj(context_vec)\n",
    "print(\"Final output shape:\", context_vec.shape)\n",
    "print(\"Final output:\\n\", context_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
